{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks for m-samples, n-features, N-neurons(hidden layer), 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"C:/Users/Anish Mulay/Documents/2nd Year/ED5340 - DS/LAB/Lab12_ed21b011-neural networks/Logistic_regression_ls.csv\")\n",
    "df = pd.read_csv(file)\n",
    "# df[\"x1\"] = df[\"x1\"]/max(df[\"x1\"])\n",
    "# df[\"x2\"] = df[\"x2\"]/max(df[\"x2\"])\n",
    "\n",
    "x_pd = df.iloc[:,:2]\n",
    "x = x_pd.to_numpy()\n",
    "\n",
    "y_pd = df.iloc[:,-1]\n",
    "y = y_pd.to_numpy()\n",
    "\n",
    "        \n",
    "        # Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0, train_size = 0.8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,neurons,dataset,X_train,Y_train):\n",
    "\n",
    "        \n",
    "        self.N = neurons\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "\n",
    "    \n",
    "class ForwardPropagation(NeuralNetwork):\n",
    "    def __init__(self,neurons,dataset,X_train, Y_train):\n",
    "        NeuralNetwork.__init__(self,neurons,dataset, X_train, Y_train)\n",
    "        self.X_train = self.X_train.T\n",
    "\n",
    "    def Z1(self,w,b):\n",
    "        self.W1 = w\n",
    "        self.b1 = b\n",
    "        self.z1 = np.matmul(self.W1,self.X_train) + self.b1\n",
    "\n",
    "    def A1(self):\n",
    "        self.a1 = 1/(1+np.exp(-self.z1))\n",
    "        # print(self.a1)\n",
    "    \n",
    "    def Z2(self,w,b):\n",
    "        self.W2 = w\n",
    "        # print(\"w2: \",self.W2)\n",
    "\n",
    "        self.b2 = b\n",
    "        # print(self.b2)\n",
    "\n",
    "        self.z2 = np.matmul(self.W2,self.a1) + self.b2\n",
    "\n",
    "    def A2(self):\n",
    "        self.a2 = 1/(1+np.exp(-self.z2))\n",
    "        # print(\"a2 : \",self.a2)\n",
    "\n",
    "class BackPropagation(ForwardPropagation):\n",
    "    def __init__(self,neurons,dataset, X_train, Y_train):\n",
    "        super().__init__(neurons,dataset, X_train, Y_train)\n",
    "        super().Z1(w = 10*np.random.rand(4,2) - 5, b = 10*np.random.rand(4,1) - 5)\n",
    "        super().A1()\n",
    "        super().Z2(w= 10*np.random.rand(1,4) - 5, b = 10*np.random.rand(1,1) - 5 )\n",
    "        super().A2()\n",
    "\n",
    "    def Derivatives(self):\n",
    "\n",
    "        self.z1 = np.matmul(self.W1,self.X_train) + self.b1\n",
    "        self.a1 = 1/(1+np.exp(-self.z1))\n",
    "        self.z2 = np.matmul(self.W2,self.a1) + self.b2\n",
    "        self.a2 = 1/(1+np.exp(-self.z2))\n",
    "        \n",
    "        self.da2 = (self.a2 - self.Y_train)/(self.a2*(1-self.a2))\n",
    "        # print(\"da2: \",self.da2)\n",
    "\n",
    "        self.dz2 = (self.a2-self.Y_train)\n",
    "\n",
    "        self.dw2 = np.matmul(self.dz2, self.a1.T) / self.X_train.shape[1]\n",
    "        \n",
    "        self.db2 = np .sum(self.dz2, axis = 1,keepdims = True)\n",
    "\n",
    "        self.dz1 = np.matmul(self.W2.T,self.dz2)*(self.a1*(1-self.a1))\n",
    "        \n",
    "        self.dw1 = np.matmul(self.dz1,self.X_train.T)/self.X_train.shape[1]\n",
    "        \n",
    "        self.db1 = np.sum(self.dz1,axis=1,keepdims=True)/self.X_train.shape[1]\n",
    "        \n",
    "    \n",
    "    def Layer1_grad(self,w2):\n",
    "        \n",
    "        self.dz1 = np.matmul(w2.T,self.dz2)*(self.a1*(1-self.a1))\n",
    "        \n",
    "        self.dw1 = np.matmul(self.dz1,self.X_train.T)/self.X_train.shape[1]\n",
    "        \n",
    "        self.db1 = np.sum(self.dz1,axis=1,keepdims=True)/self.X_train.shape[1]\n",
    "        return self.dw1\n",
    "    \n",
    "    def gradient_descent(self,learning_rate=0.03):\n",
    "        self.W1 -= learning_rate * self.dw1\n",
    "        self.W2 = self.W2 - learning_rate * self.dw2\n",
    "        self.b1 -= learning_rate * self.db1\n",
    "        \n",
    "        self.b2 -= learning_rate * self.db2\n",
    "\n",
    "\n",
    "    def constAlpha(self,iterations,alpha, what_to_optimize, grad_l2):\n",
    "        l2 = what_to_optimize\n",
    "        w1 = self.W1\n",
    "        for i in range(iterations):\n",
    "                l2 = l2 - alpha * grad_l2\n",
    "                \n",
    "                dw1 = self.Layer1_grad(l2)\n",
    "                \n",
    "                w1 = w1 - alpha * dw1\n",
    "                \n",
    "\n",
    "                i+=1\n",
    "                # return p\n",
    "        return l2,w1\n",
    "    \n",
    "    def constAlpha_for_bias(self,iterations,alpha, what_to_optimize, grad_l2):\n",
    "        l2 = what_to_optimize\n",
    "        print(\"b2 ka grad: \",grad_l2)\n",
    "        print(\"mera b2: \", l2)\n",
    "        for i in range(iterations):\n",
    "                l2 = l2 - alpha * grad_l2.T\n",
    "                i+=1\n",
    "        return l2\n",
    "        \n",
    "    \n",
    "    def FinalValues(self,iterations):\n",
    "        i =0\n",
    "        self.Derivatives()\n",
    "        while i<iterations:\n",
    "            i+=1\n",
    "            self.gradient_descent()\n",
    "            self.Derivatives()\n",
    "            # print(f\"\\ndw1 = {self.dw1}\")\n",
    "            # print(f\"\\ndw2 = {self.dw2}\\n\")\n",
    "        self.final_W2 = self.W2\n",
    "        # print(f\"Final W2: \", self.final_W2)\n",
    "        \n",
    "        self.final_b2 = self.b2\n",
    "        # print(\"Final b2: \",self.final_b2.shape)\n",
    "        # print(\"b2: \",self.b2)\n",
    "\n",
    "        self.final_w1 = self.W1\n",
    "        # print(\"Final w1: \",self.final_w1)\n",
    "\n",
    "        self.final_b1 = self.W1\n",
    "        # print(\"Final b1: \",self.final_b1)\n",
    "\n",
    "        # self.W1 = self.final_w1\n",
    "        # self.b1 = self.final_b1\n",
    "        self.z1 = np.matmul(self.W1,self.X_train) + self.b1\n",
    "        self.a1 = 1/(1+np.exp(-self.z1))\n",
    "        self.W2 = self.final_W2\n",
    "        self.b2 = self.final_b2\n",
    "        self.z2 = np.matmul(self.W2,self.a1) + self.b2\n",
    "        # print(\"final z2: \",self.z2)\n",
    "        self.a2 = 1/(1+np.exp(-self.z2))\n",
    "        \n",
    "        for i in range(len(self.a2[0])):\n",
    "            if self.a2[0][i] > 0.5:\n",
    "                self.a2[0][i] = 1\n",
    "            else: \n",
    "                self.a2[0][i] = 0\n",
    "        # print(f\"self.a2 = {self.a2}\")\n",
    "        \n",
    "\n",
    "    def CostFunction(self,):\n",
    "        self.J = -self.Y_train*np.log(self.a2) - (1-self.Y_train)*np.log(1-self.a2)\n",
    "        print(\"j: \",self.J)\n",
    "\n",
    "    def Test(self,X_test,Y_test):\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        \n",
    "        self.z1_test = np.matmul(self.final_w1,self.X_test) + self.final_b2\n",
    "        self.a1_test = 1/(1+np.exp(-self.z1_test))\n",
    "        \n",
    "        self.z2_test= np.matmul(self.final_W2,self.a1_test) + self.final_b2\n",
    "        # print(\"final z2: \",self.z2)\n",
    "        self.a2_test = 1/(1+np.exp(-self.z2_test))\n",
    "\n",
    "        for i in range(len(self.a2_test[0])):\n",
    "            if self.a2_test[0][i] > 0.5:\n",
    "                self.a2_test[0][i] = 1\n",
    "            else: \n",
    "                self.a2_test[0][i] = 0\n",
    "        # print(f\"self.a2 = {self.a2}\")\n",
    "\n",
    "        print(\"test samples predicted: \",self.a2_test)\n",
    "\n",
    "        tn=0\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        for predicted_value, test_value in zip(self.a2_test[0],self.Y_test):\n",
    "            predicted_value = int(predicted_value)\n",
    "            if predicted_value == test_value == 0:\n",
    "                tn+=1\n",
    "            elif predicted_value == test_value == 1:\n",
    "                tp+=1\n",
    "            elif predicted_value ==0 and test_value == 1:\n",
    "                fn+=1\n",
    "            \n",
    "            elif predicted_value ==1 and test_value == 0:\n",
    "                fp+=1\n",
    "        \n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "        \n",
    "        print(\"Precision: \", precision, \" Recall: \", recall, \" Accuracy: \",accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# obj = ForwardPropagation(neurons = 4,dataset = file, X_train = X_train,Y_train = y_train)\n",
    "# obj.Z1(w = np.array([[0.1,0.5]]),b = 0)\n",
    "# obj.A1()\n",
    "# obj.Z2(w= np.array([[0.1,0.1,0.2,0.3]]),b = 0.1)\n",
    "# obj.A2()\n",
    "\n",
    "obj2 = BackPropagation(neurons = 6,dataset = file, X_train = X_train, Y_train = y_train)\n",
    "obj2.Derivatives()\n",
    "obj2.FinalValues(iterations=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(obj2.Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test samples predicted:  [[1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0.\n",
      "  0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      "  0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0.\n",
      "  1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      "  1. 0. 1. 1.]]\n",
      "Precision:  1.0  Recall:  0.9803921568627451  Accuracy:  0.99\n"
     ]
    }
   ],
   "source": [
    "obj2.Test(X_test.T, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
      " 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0\n",
      " 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
